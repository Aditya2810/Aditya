{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\adity\\\\Downloads\\\\ML'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "np.random.seed(10)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
       "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
       "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
       "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
       "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
       "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ENB2012_data.csv', na_values=['?'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764167</td>\n",
       "      <td>671.708333</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>176.604167</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.81250</td>\n",
       "      <td>22.307201</td>\n",
       "      <td>24.587760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105777</td>\n",
       "      <td>88.086116</td>\n",
       "      <td>43.626481</td>\n",
       "      <td>45.165950</td>\n",
       "      <td>1.75114</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.133221</td>\n",
       "      <td>1.55096</td>\n",
       "      <td>10.090196</td>\n",
       "      <td>9.513306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.010000</td>\n",
       "      <td>10.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.682500</td>\n",
       "      <td>606.375000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>140.875000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>12.992500</td>\n",
       "      <td>15.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>673.750000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>183.750000</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>18.950000</td>\n",
       "      <td>22.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>741.125000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>31.667500</td>\n",
       "      <td>33.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>48.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4         X5          X6  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
       "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
       "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
       "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
       "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
       "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
       "\n",
       "               X7         X8          Y1          Y2  \n",
       "count  768.000000  768.00000  768.000000  768.000000  \n",
       "mean     0.234375    2.81250   22.307201   24.587760  \n",
       "std      0.133221    1.55096   10.090196    9.513306  \n",
       "min      0.000000    0.00000    6.010000   10.900000  \n",
       "25%      0.100000    1.75000   12.992500   15.620000  \n",
       "50%      0.250000    3.00000   18.950000   22.080000  \n",
       "75%      0.400000    4.00000   31.667500   33.132500  \n",
       "max      0.400000    5.00000   43.100000   48.030000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1    0.0\n",
      "X2    0.0\n",
      "X3    0.0\n",
      "X4    0.0\n",
      "X5    0.0\n",
      "X6    0.0\n",
      "X7    0.0\n",
      "X8    0.0\n",
      "Y1    0.0\n",
      "Y2    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "percentage_missing = (data.isnull().sum()*100)/(len(data))\n",
    "print(percentage_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining X and Y variables\n",
    "x = data.drop([\"Y1\",\"Y2\"], axis = 1)\n",
    "y1 = data[\"Y1\"]\n",
    "y2 = data[\"Y2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictng for Y1 - Heating Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomising data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y1_train, y1_test = train_test_split(x, y1, test_size = 0.1, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_x = MinMaxScaler()\n",
    "x_train = mm_x.fit_transform(x_train)\n",
    "x_test = mm_x.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries for Score and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R-squared score (training) for Heating Load: 0.914\n",
      "Linear Regression R-squared score (test)  for Heating Load: 0.933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "linReg_heat = regressor.fit(x_train, y1_train)\n",
    "\n",
    "print('Linear Regression R-squared score (training) for Heating Load: {:.3f}'\n",
    "     .format(linReg_heat.score(x_train, y1_train)))\n",
    "print('Linear Regression R-squared score (test)  for Heating Load: {:.3f}'\n",
    "     .format(linReg_heat.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'C': 100, 'gamma': 0.001}\n",
      "Best cross_val score : 0.9077288977147542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel = 'linear')\n",
    "param_grid = {'C':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100],\n",
    "              'gamma' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100]}\n",
    "grid_search3= GridSearchCV(svr,param_grid, cv=5)\n",
    "grid_search3.fit(x_train, y1_train)\n",
    "\n",
    "print(\"Best Parameters : {}\".format(grid_search3.best_params_))\n",
    "print(\"Best cross_val score : {}\".format(grid_search3.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Linear R-squared score (training) for Heating Load: 0.911\n",
      "SVM Linear R-squared score (test) for Heating Load: 0.934\n"
     ]
    }
   ],
   "source": [
    "svrlin = SVR(kernel = 'linear', C = 100, gamma = 0.001)\n",
    "svrlin_heat = svrlin.fit(x_train,y1_train)\n",
    "print('SVM Linear R-squared score (training) for Heating Load: {:.3f}'\n",
    "     .format(svrlin_heat.score(x_train, y1_train)))\n",
    "print('SVM Linear R-squared score (test) for Heating Load: {:.3f}'\n",
    "     .format(svrlin_heat.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine - Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'C': 50, 'gamma': 10}\n",
      "Best cross_val score : 0.9621492547474835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr2 = SVR(kernel = 'rbf')\n",
    "param_grid = {'C':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100],\n",
    "              'gamma' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100]}\n",
    "grid_search4= GridSearchCV(svr2,param_grid, cv=5)\n",
    "grid_search4.fit(x_train, y1_train)\n",
    "\n",
    "print(\"Best Parameters : {}\".format(grid_search4.best_params_))\n",
    "print(\"Best cross_val score : {}\".format(grid_search4.best_score_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 1.000\n",
      "R-_squared score (test): 0.972\n"
     ]
    }
   ],
   "source": [
    "svrrbf= SVR(kernel = 'rbf', C = 50, gamma = 10)\n",
    "svrrbf_heat = svrrbf.fit(x_train,y1_train)\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(svrrbf_heat.score(x_train, y1_train)))\n",
    "print('R-_squared score (test): {:.3f}'\n",
    "     .format(svrrbf_heat.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knn Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for KNN repressor - Heating Load: {'n_neighbors': 7}\n",
      "Best Score for KNN repressor - Heating Load: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knnreg = KNeighborsRegressor()\n",
    "param_grid = {'n_neighbors': [3, 4, 5, 6, 7]}\n",
    "grid_search_knnreg = GridSearchCV(knnreg, param_grid, cv = 5, scoring = 'r2')\n",
    "grid_search_knnreg.fit(x_train, y1_train)\n",
    "print(\"Best Parameters for KNN repressor - Heating Load: {}\".format(grid_search_knnreg.best_params_))\n",
    "print(\"Best Score for KNN repressor - Heating Load: {:.2f}\".format(grid_search_knnreg.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.942\n",
      "R-squared score (test): 0.932\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=7)\n",
    "knnReg = knn.fit(x_train,y1_train)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(knnReg.score(x_train, y1_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(knnReg.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After running OLS, SVR(Linear and Kernel), and KNN regressors, we can say that Linear SVR and OLS give very good results. Let us try bagging and boosting methods for these models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'learning_rate': 0.1, 'n_estimators': 500}\n",
      "Best cross_val score : 0.9619858992725867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "adaboost = AdaBoostRegressor(random_state = 10)\n",
    "param_grid = {'n_estimators':[10,50,100,200,500,750,1000],\n",
    "              'learning_rate':[0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]}\n",
    "grid_search_adaboost= GridSearchCV(adaboost,param_grid, cv=5, scoring = 'r2')\n",
    "grid_search_adaboost.fit(x_train, y1_train)\n",
    "\n",
    "print(\"Best Parameters : {}\".format(grid_search_adaboost.best_params_))\n",
    "print(\"Best cross_val score : {}\".format(grid_search_adaboost.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost for Linear Regression on training set: 0.91\n",
      "Adaboost for Linear Regression on test set: 0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_lin = AdaBoostRegressor(regressor, learning_rate = 0.1, n_estimators = 500, random_state = 10)\n",
    "ada_lin.fit(x_train, y1_train)\n",
    "\n",
    "print('Adaboost for Linear Regression on training set: {:.2f}'\n",
    "     .format(ada_lin.score(x_train, y1_train)))\n",
    "print('Adaboost for Linear Regression on test set: {:.2f}\\n'\n",
    "     .format(ada_lin.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost for Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost for Linear SVM on training set: 0.91\n",
      "Adaboost for Linear SVM on test set: 0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_svrlin = AdaBoostRegressor(svrlin, learning_rate = 0.1, n_estimators = 500, random_state = 10)\n",
    "ada_svrlin.fit(x_train, y1_train)\n",
    "\n",
    "print('Adaboost for Linear SVM on training set: {:.2f}'\n",
    "     .format(ada_svrlin.score(x_train, y1_train)))\n",
    "print('Adaboost for Linear SVM on test set: {:.2f}\\n'\n",
    "     .format(ada_svrlin.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost for Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost for Kernel SVM on training set: 1.00\n",
      "Adaboost for Kernel SVM on test set: 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_svrrbf = AdaBoostRegressor(svrrbf, learning_rate = 0.1, n_estimators = 500, random_state = 10)\n",
    "ada_svrrbf.fit(x_train, y1_train)\n",
    "\n",
    "print('Adaboost for Kernel SVM on training set: {:.2f}'\n",
    "     .format(ada_svrrbf.score(x_train, y1_train)))\n",
    "print('Adaboost for Kernel SVM on test set: {:.2f}\\n'\n",
    "     .format(ada_svrrbf.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost for KNN on training set: 0.95\n",
      "Adaboost for KNN on test set: 0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_knn = AdaBoostRegressor(knn, learning_rate = 0.1, n_estimators = 500, random_state = 10)\n",
    "ada_knn.fit(x_train, y1_train)\n",
    "\n",
    "print('Adaboost for KNN on training set: {:.2f}'\n",
    "     .format(ada_knn.score(x_train, y1_train)))\n",
    "print('Adaboost for KNN on test set: {:.2f}\\n'\n",
    "     .format(ada_knn.score(x_test, y1_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost gives very good results for KNN compared to all other models. The train scores for OLS and Linear regression is less compared to KNN for the same test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'max_samples': 100}\n",
      "Best cross_val score : 0.9906340747620854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagging = BaggingRegressor(random_state = 10)\n",
    "param_grid = {'max_samples':[10,50,100]}\n",
    "grid_search_bagging = GridSearchCV(bagging, param_grid, cv=5, scoring = 'r2')\n",
    "grid_search_bagging.fit(x_train, y1_train)\n",
    "\n",
    "print(\"Best Parameters : {}\".format(grid_search_bagging.best_params_))\n",
    "print(\"Best cross_val score : {}\".format(grid_search_bagging.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging for Linear Regression on training set: 0.91\n",
      "Bagging for Linear Regression on test set: 0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_lin= BaggingRegressor(regressor, max_samples = 100, random_state = 10)\n",
    "bag_lin.fit(x_train, y1_train)\n",
    "\n",
    "print('Bagging for Linear Regression on training set: {:.2f}'\n",
    "     .format(bag_lin.score(x_train, y1_train)))\n",
    "print('Bagging for Linear Regression on test set: {:.2f}\\n'\n",
    "     .format(bag_lin.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging for Linear SVM on training set: 0.91\n",
      "Bagging for Linear SVM on test set: 0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_svrlin= BaggingRegressor(svrlin, max_samples = 100, random_state = 10)\n",
    "bag_svrlin.fit(x_train, y1_train)\n",
    "\n",
    "print('Bagging for Linear SVM on training set: {:.2f}'\n",
    "     .format(bag_svrlin.score(x_train, y1_train)))\n",
    "print('Bagging for Linear SVM on test set: {:.2f}\\n'\n",
    "     .format(bag_svrlin.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging for Kernel SVM on training set: 0.77\n",
      "Bagging for Kernel SVM on test set: 0.68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_svrrbf= BaggingRegressor(svrrbf, max_samples = 100, random_state = 10)\n",
    "bag_svrrbf.fit(x_train, y1_train)\n",
    "\n",
    "print('Bagging for Kernel SVM on training set: {:.2f}'\n",
    "     .format(bag_svrrbf.score(x_train, y1_train)))\n",
    "print('Bagging for Kernel SVM on test set: {:.2f}\\n'\n",
    "     .format(bag_svrrbf.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging for Kernel SVM on training set: 0.90\n",
      "Bagging for Kernel SVM on test set: 0.91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_knn= BaggingRegressor(knn, max_samples = 100, random_state = 10)\n",
    "bag_knn.fit(x_train, y1_train)\n",
    "\n",
    "print('Bagging for Kernel SVM on training set: {:.2f}'\n",
    "     .format(bag_knn.score(x_train, y1_train)))\n",
    "print('Bagging for Kernel SVM on test set: {:.2f}\\n'\n",
    "     .format(bag_knn.score(x_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at all the models built using ensemble methods, we can see that the scores of OLS and Linear SVR are consistent. We can use Linear SVR to predict the value of y1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.91\n",
      "Test r2: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y1_train_predict_svclin = bag_svrlin.predict(x_train)\n",
    "y1_test_predict_svclin = bag_svrlin.predict(x_test)\n",
    "\n",
    "print('Train r2: {:.2f}'.format(r2_score(y1_train, y1_train_predict_svclin)))\n",
    "print('Test r2: {:.2f}'.format(r2_score(y1_test, y1_test_predict_svclin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 8)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "691/691 [==============================] - 3s 4ms/step - loss: 601.0948 - mean_squared_error: 601.0948\n",
      "Epoch 2/100\n",
      "691/691 [==============================] - 0s 118us/step - loss: 597.1087 - mean_squared_error: 597.1087\n",
      "Epoch 3/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 585.9691 - mean_squared_error: 585.9691\n",
      "Epoch 4/100\n",
      "691/691 [==============================] - 0s 139us/step - loss: 558.2864 - mean_squared_error: 558.2864\n",
      "Epoch 5/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 504.5510 - mean_squared_error: 504.5510\n",
      "Epoch 6/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 422.2890 - mean_squared_error: 422.2890\n",
      "Epoch 7/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 319.7315 - mean_squared_error: 319.7315\n",
      "Epoch 8/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 217.9224 - mean_squared_error: 217.9224\n",
      "Epoch 9/100\n",
      "691/691 [==============================] - 0s 363us/step - loss: 139.8061 - mean_squared_error: 139.8061\n",
      "Epoch 10/100\n",
      "691/691 [==============================] - 0s 231us/step - loss: 96.7890 - mean_squared_error: 96.7890\n",
      "Epoch 11/100\n",
      "691/691 [==============================] - 0s 157us/step - loss: 78.4430 - mean_squared_error: 78.4430\n",
      "Epoch 12/100\n",
      "691/691 [==============================] - 0s 168us/step - loss: 69.4033 - mean_squared_error: 69.4033\n",
      "Epoch 13/100\n",
      "691/691 [==============================] - 0s 142us/step - loss: 63.1546 - mean_squared_error: 63.1546\n",
      "Epoch 14/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 57.2905 - mean_squared_error: 57.2905\n",
      "Epoch 15/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 52.1048 - mean_squared_error: 52.1048\n",
      "Epoch 16/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 47.4081 - mean_squared_error: 47.4081\n",
      "Epoch 17/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 42.8143 - mean_squared_error: 42.8143\n",
      "Epoch 18/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 38.0191 - mean_squared_error: 38.0191\n",
      "Epoch 19/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 33.3877 - mean_squared_error: 33.3877\n",
      "Epoch 20/100\n",
      "691/691 [==============================] - 0s 292us/step - loss: 29.2535 - mean_squared_error: 29.25350s - loss: 28.7044 - mean_squared_error: 28.\n",
      "Epoch 21/100\n",
      "691/691 [==============================] - 0s 308us/step - loss: 25.7061 - mean_squared_error: 25.7061\n",
      "Epoch 22/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 22.7430 - mean_squared_error: 22.7430\n",
      "Epoch 23/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 20.4460 - mean_squared_error: 20.4460\n",
      "Epoch 24/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 18.5516 - mean_squared_error: 18.5516\n",
      "Epoch 25/100\n",
      "691/691 [==============================] - 0s 115us/step - loss: 17.1115 - mean_squared_error: 17.1115\n",
      "Epoch 26/100\n",
      "691/691 [==============================] - 0s 142us/step - loss: 16.0104 - mean_squared_error: 16.0104\n",
      "Epoch 27/100\n",
      "691/691 [==============================] - 0s 142us/step - loss: 15.2232 - mean_squared_error: 15.2232\n",
      "Epoch 28/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 14.6376 - mean_squared_error: 14.6376\n",
      "Epoch 29/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 14.1431 - mean_squared_error: 14.1431\n",
      "Epoch 30/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 13.7548 - mean_squared_error: 13.7548\n",
      "Epoch 31/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 13.4826 - mean_squared_error: 13.4826\n",
      "Epoch 32/100\n",
      "691/691 [==============================] - 0s 524us/step - loss: 13.1984 - mean_squared_error: 13.1984\n",
      "Epoch 33/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 12.9284 - mean_squared_error: 12.9284\n",
      "Epoch 34/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 12.7187 - mean_squared_error: 12.7187\n",
      "Epoch 35/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 12.5141 - mean_squared_error: 12.5141\n",
      "Epoch 36/100\n",
      "691/691 [==============================] - 0s 139us/step - loss: 12.3299 - mean_squared_error: 12.3299\n",
      "Epoch 37/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 12.1523 - mean_squared_error: 12.1523\n",
      "Epoch 38/100\n",
      "691/691 [==============================] - 0s 142us/step - loss: 11.9928 - mean_squared_error: 11.9928\n",
      "Epoch 39/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 11.8385 - mean_squared_error: 11.8385\n",
      "Epoch 40/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 11.7107 - mean_squared_error: 11.7107\n",
      "Epoch 41/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 11.5569 - mean_squared_error: 11.5569\n",
      "Epoch 42/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 11.4417 - mean_squared_error: 11.4417\n",
      "Epoch 43/100\n",
      "691/691 [==============================] - 0s 454us/step - loss: 11.3104 - mean_squared_error: 11.3104\n",
      "Epoch 44/100\n",
      "691/691 [==============================] - 0s 156us/step - loss: 11.1982 - mean_squared_error: 11.1982\n",
      "Epoch 45/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 11.1036 - mean_squared_error: 11.1036\n",
      "Epoch 46/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 11.0107 - mean_squared_error: 11.0107\n",
      "Epoch 47/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 10.9202 - mean_squared_error: 10.9202\n",
      "Epoch 48/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 10.8561 - mean_squared_error: 10.8561\n",
      "Epoch 49/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 10.7453 - mean_squared_error: 10.7453\n",
      "Epoch 50/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 10.6953 - mean_squared_error: 10.6953\n",
      "Epoch 51/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 10.6091 - mean_squared_error: 10.6091\n",
      "Epoch 52/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 10.5724 - mean_squared_error: 10.5724\n",
      "Epoch 53/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 10.4822 - mean_squared_error: 10.4822\n",
      "Epoch 54/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 10.4281 - mean_squared_error: 10.4281\n",
      "Epoch 55/100\n",
      "691/691 [==============================] - 0s 337us/step - loss: 10.3615 - mean_squared_error: 10.3615\n",
      "Epoch 56/100\n",
      "691/691 [==============================] - 0s 248us/step - loss: 10.3236 - mean_squared_error: 10.3236\n",
      "Epoch 57/100\n",
      "691/691 [==============================] - 0s 165us/step - loss: 10.2935 - mean_squared_error: 10.2935\n",
      "Epoch 58/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 10.2242 - mean_squared_error: 10.2242\n",
      "Epoch 59/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 10.1838 - mean_squared_error: 10.1838\n",
      "Epoch 60/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 10.1589 - mean_squared_error: 10.1589\n",
      "Epoch 61/100\n",
      "691/691 [==============================] - 0s 106us/step - loss: 10.1247 - mean_squared_error: 10.1247\n",
      "Epoch 62/100\n",
      "691/691 [==============================] - 0s 115us/step - loss: 10.0814 - mean_squared_error: 10.0814\n",
      "Epoch 63/100\n",
      "691/691 [==============================] - 0s 118us/step - loss: 10.0333 - mean_squared_error: 10.0333\n",
      "Epoch 64/100\n",
      "691/691 [==============================] - 0s 119us/step - loss: 10.0200 - mean_squared_error: 10.0200\n",
      "Epoch 65/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 9.9561 - mean_squared_error: 9.9561\n",
      "Epoch 66/100\n",
      "691/691 [==============================] - 0s 164us/step - loss: 9.9117 - mean_squared_error: 9.9117\n",
      "Epoch 67/100\n",
      "691/691 [==============================] - 0s 252us/step - loss: 9.8913 - mean_squared_error: 9.8913\n",
      "Epoch 68/100\n",
      "691/691 [==============================] - 0s 196us/step - loss: 9.8833 - mean_squared_error: 9.8833\n",
      "Epoch 69/100\n",
      "691/691 [==============================] - 0s 110us/step - loss: 9.8127 - mean_squared_error: 9.8127\n",
      "Epoch 70/100\n",
      "691/691 [==============================] - 0s 116us/step - loss: 9.7995 - mean_squared_error: 9.7995\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 145us/step - loss: 9.8465 - mean_squared_error: 9.8465\n",
      "Epoch 72/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 9.7429 - mean_squared_error: 9.7429\n",
      "Epoch 73/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 9.7090 - mean_squared_error: 9.7090\n",
      "Epoch 74/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 9.6962 - mean_squared_error: 9.6962\n",
      "Epoch 75/100\n",
      "691/691 [==============================] - 0s 113us/step - loss: 9.6367 - mean_squared_error: 9.6367\n",
      "Epoch 76/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 9.6227 - mean_squared_error: 9.6227\n",
      "Epoch 77/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 9.5987 - mean_squared_error: 9.5987\n",
      "Epoch 78/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 9.5761 - mean_squared_error: 9.5761\n",
      "Epoch 79/100\n",
      "691/691 [==============================] - 0s 241us/step - loss: 9.5349 - mean_squared_error: 9.5349\n",
      "Epoch 80/100\n",
      "691/691 [==============================] - 0s 279us/step - loss: 9.5061 - mean_squared_error: 9.5061\n",
      "Epoch 81/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 9.5248 - mean_squared_error: 9.5248\n",
      "Epoch 82/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 9.5364 - mean_squared_error: 9.5364\n",
      "Epoch 83/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 9.4393 - mean_squared_error: 9.4393\n",
      "Epoch 84/100\n",
      "691/691 [==============================] - 0s 168us/step - loss: 9.4238 - mean_squared_error: 9.4238\n",
      "Epoch 85/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 9.4352 - mean_squared_error: 9.4352\n",
      "Epoch 86/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 9.3863 - mean_squared_error: 9.3863\n",
      "Epoch 87/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 9.3576 - mean_squared_error: 9.3576\n",
      "Epoch 88/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 9.3635 - mean_squared_error: 9.3635\n",
      "Epoch 89/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 9.3561 - mean_squared_error: 9.3561\n",
      "Epoch 90/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 9.2892 - mean_squared_error: 9.2892\n",
      "Epoch 91/100\n",
      "691/691 [==============================] - 0s 294us/step - loss: 9.2540 - mean_squared_error: 9.2540\n",
      "Epoch 92/100\n",
      "691/691 [==============================] - 0s 231us/step - loss: 9.2539 - mean_squared_error: 9.2539\n",
      "Epoch 93/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 9.2215 - mean_squared_error: 9.2215\n",
      "Epoch 94/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 9.2051 - mean_squared_error: 9.2051\n",
      "Epoch 95/100\n",
      "691/691 [==============================] - 0s 112us/step - loss: 9.1466 - mean_squared_error: 9.1466\n",
      "Epoch 96/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 9.1303 - mean_squared_error: 9.1303\n",
      "Epoch 97/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 9.1188 - mean_squared_error: 9.1188\n",
      "Epoch 98/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 9.0943 - mean_squared_error: 9.0943\n",
      "Epoch 99/100\n",
      "691/691 [==============================] - 0s 109us/step - loss: 9.1279 - mean_squared_error: 9.1279\n",
      "Epoch 100/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 9.1098 - mean_squared_error: 9.1098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e44b94e1d0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=8, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(6, kernel_initializer = 'normal', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='adam' , metrics = ['mse'])\n",
    "\n",
    "model.fit(x_train, y1_train, epochs = 100, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.91\n",
      "Test r2: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y1_train_predict = model.predict(x_train)\n",
    "y1_test_predict = model.predict(x_test)\n",
    "\n",
    "print('Train r2: {:.2f}'.format(r2_score(y1_train, y1_train_predict)))\n",
    "print('Test r2: {:.2f}'.format(r2_score(y1_test, y1_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to all the models, linear SVR is more robust and gives high test scores with comparable training scores. Hence, we can use Linear SVR to predict y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictng for Y2 - Cooling Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomising data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y2_train, y2_test = train_test_split(x, y2, test_size = 0.1, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_x = MinMaxScaler()\n",
    "x_train = mm_x.fit_transform(x_train)\n",
    "x_test = mm_x.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries for Score and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R-squared score (training) for Heating Load: 0.885\n",
      "Linear Regression R-squared score (test)  for Heating Load: 0.909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "linReg_heat = regressor.fit(x_train, y2_train)\n",
    "\n",
    "print('Linear Regression R-squared score (training) for Heating Load: {:.3f}'\n",
    "     .format(linReg_heat.score(x_train, y2_train)))\n",
    "print('Linear Regression R-squared score (test)  for Heating Load: {:.3f}'\n",
    "     .format(linReg_heat.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'C': 10, 'gamma': 0.001}\n",
      "Best cross_val score : 0.8755295597031377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel = 'linear')\n",
    "param_grid = {'C':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100],\n",
    "              'gamma' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100]}\n",
    "grid_search3= GridSearchCV(svr,param_grid, cv=5)\n",
    "grid_search3.fit(x_train, y2_train)\n",
    "\n",
    "print(\"Best Parameters : {}\".format(grid_search3.best_params_))\n",
    "print(\"Best cross_val score : {}\".format(grid_search3.best_score_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Linear R-squared score (training) for Heating Load: 0.877\n",
      "SVM Linear R-squared score (test) for Heating Load: 0.908\n"
     ]
    }
   ],
   "source": [
    "svrlin = SVR(kernel = 'linear', C = 10, gamma = 0.001)\n",
    "svrlin_heat = svrlin.fit(x_train,y2_train)\n",
    "print('SVM Linear R-squared score (training) for Heating Load: {:.3f}'\n",
    "     .format(svrlin_heat.score(x_train, y2_train)))\n",
    "print('SVM Linear R-squared score (test) for Heating Load: {:.3f}'\n",
    "     .format(svrlin_heat.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine - Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'C': 100, 'gamma': 10}\n",
      "Best cross_val score : 0.9414908740283418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr2 = SVR(kernel = 'rbf')\n",
    "param_grid = {'C':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100],\n",
    "              'gamma' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100]}\n",
    "grid_search4= GridSearchCV(svr2,param_grid, cv=5)\n",
    "grid_search4.fit(x_train, y2_train)\n",
    "\n",
    "print(\"Best Parameters : {}\".format(grid_search4.best_params_))\n",
    "print(\"Best cross_val score : {}\".format(grid_search4.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 1.000\n",
      "R-_squared score (test): 0.947\n"
     ]
    }
   ],
   "source": [
    "svrrbf= SVR(kernel = 'rbf', C = 100, gamma = 10)\n",
    "svrrbf_cool = svrrbf.fit(x_train,y2_train)\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(svrrbf_cool.score(x_train, y2_train)))\n",
    "print('R-_squared score (test): {:.3f}'\n",
    "     .format(svrrbf_cool.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knn Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_neighbors': 7}\n",
      "Best Score for KNN repressor - Cooling Load: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knnreg = KNeighborsRegressor()\n",
    "param_grid = {'n_neighbors': [3, 4, 5, 6, 7]}\n",
    "grid_search_knnreg = GridSearchCV(knnreg, param_grid, cv = 5, scoring = 'r2')\n",
    "grid_search_knnreg.fit(x_train, y2_train)\n",
    "print(\"Best Parameters: {}\".format(grid_search_knnreg.best_params_))\n",
    "print(\"Best Score for KNN repressor - Cooling Load: {:.2f}\".format(grid_search_knnreg.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.923\n",
      "R-squared score (test): 0.925\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=7)\n",
    "knnReg = knn.fit(x_train,y2_train)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(knnReg.score(x_train, y2_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(knnReg.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN gives high and comparable train and test scores compared to all other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "Best cross_val score : 0.9326375047259011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "adaboost = AdaBoostRegressor(random_state = 10)\n",
    "param_grid = {'n_estimators':[10,50,100,200,500,750,1000],\n",
    "              'learning_rate':[0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]}\n",
    "grid_search_adaboost= GridSearchCV(adaboost,param_grid, cv=5, scoring = 'r2')\n",
    "grid_search_adaboost.fit(x_train, y2_train)\n",
    "\n",
    "print(\"Best Parameters : {}\".format(grid_search_adaboost.best_params_))\n",
    "print(\"Best cross_val score : {}\".format(grid_search_adaboost.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost for Linear Regression on training set: 0.88\n",
      "Adaboost for Linear Regression on test set: 0.90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_lin = AdaBoostRegressor(regressor, learning_rate = 0.1, n_estimators = 200, random_state = 10)\n",
    "ada_lin.fit(x_train, y2_train)\n",
    "\n",
    "print('Adaboost for Linear Regression on training set: {:.2f}'\n",
    "     .format(ada_lin.score(x_train, y2_train)))\n",
    "print('Adaboost for Linear Regression on test set: {:.2f}\\n'\n",
    "     .format(ada_lin.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost for Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost for Linear SVM on training set: 0.88\n",
      "Adaboost for Linear SVM on test set: 0.90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_svrlin = AdaBoostRegressor(svrlin, learning_rate = 0.1, n_estimators = 200, random_state = 10)\n",
    "ada_svrlin.fit(x_train, y2_train)\n",
    "\n",
    "print('Adaboost for Linear SVM on training set: {:.2f}'\n",
    "     .format(ada_svrlin.score(x_train, y2_train)))\n",
    "print('Adaboost for Linear SVM on test set: {:.2f}\\n'\n",
    "     .format(ada_svrlin.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost for Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost for Kernel SVM on training set: 1.00\n",
      "Adaboost for Kernel SVM on test set: 0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_svrrbf = AdaBoostRegressor(svrrbf, learning_rate = 0.1, n_estimators = 200, random_state = 10)\n",
    "ada_svrrbf.fit(x_train, y2_train)\n",
    "\n",
    "print('Adaboost for Kernel SVM on training set: {:.2f}'\n",
    "     .format(ada_svrrbf.score(x_train, y2_train)))\n",
    "print('Adaboost for Kernel SVM on test set: {:.2f}\\n'\n",
    "     .format(ada_svrrbf.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost for KNN on training set: 0.95\n",
      "Adaboost for KNN on test set: 0.92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_knn = AdaBoostRegressor(knn, learning_rate = 0.1, n_estimators = 200, random_state = 10)\n",
    "ada_knn.fit(x_train, y2_train)\n",
    "\n",
    "print('Adaboost for KNN on training set: {:.2f}'\n",
    "     .format(ada_knn.score(x_train, y2_train)))\n",
    "print('Adaboost for KNN on test set: {:.2f}\\n'\n",
    "     .format(ada_knn.score(x_test, y2_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With comparable train and test scores, KNN comes as the best model for ADAboost as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'max_samples': 100}\n",
      "Best cross_val score : 0.957887237332568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagging = BaggingRegressor(random_state = 10)\n",
    "param_grid = {'max_samples':[10,50,100]}\n",
    "grid_search_bagging = GridSearchCV(bagging, param_grid, cv=5, scoring = 'r2')\n",
    "grid_search_bagging.fit(x_train, y2_train)\n",
    "\n",
    "print(\"Best Parameters : {}\".format(grid_search_bagging.best_params_))\n",
    "print(\"Best cross_val score : {}\".format(grid_search_bagging.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging for Linear Regression on training set: 0.88\n",
      "Bagging for Linear Regression on test set: 0.90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_lin= BaggingRegressor(regressor, max_samples = 100, random_state = 10)\n",
    "bag_lin.fit(x_train, y2_train)\n",
    "\n",
    "print('Bagging for Linear Regression on training set: {:.2f}'\n",
    "     .format(bag_lin.score(x_train, y2_train)))\n",
    "print('Bagging for Linear Regression on test set: {:.2f}\\n'\n",
    "     .format(bag_lin.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging for Linear SVM on training set: 0.86\n",
      "Bagging for Linear SVM on test set: 0.90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_svrlin= BaggingRegressor(svrlin, max_samples = 100, random_state = 10)\n",
    "bag_svrlin.fit(x_train, y2_train)\n",
    "\n",
    "print('Bagging for Linear SVM on training set: {:.2f}'\n",
    "     .format(bag_svrlin.score(x_train, y2_train)))\n",
    "print('Bagging for Linear SVM on test set: {:.2f}\\n'\n",
    "     .format(bag_svrlin.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging for Kernel SVM on training set: 0.75\n",
      "Bagging for Kernel SVM on test set: 0.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_svrrbf= BaggingRegressor(svrrbf, max_samples = 100, random_state = 10)\n",
    "bag_svrrbf.fit(x_train, y2_train)\n",
    "\n",
    "print('Bagging for Kernel SVM on training set: {:.2f}'\n",
    "     .format(bag_svrrbf.score(x_train, y2_train)))\n",
    "print('Bagging for Kernel SVM on test set: {:.2f}\\n'\n",
    "     .format(bag_svrrbf.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging for Kernel SVM on training set: 0.88\n",
      "Bagging for Kernel SVM on test set: 0.90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bag_knn= BaggingRegressor(knn, max_samples = 100, random_state = 10)\n",
    "bag_knn.fit(x_train, y2_train)\n",
    "\n",
    "print('Bagging for Kernel SVM on training set: {:.2f}'\n",
    "     .format(bag_knn.score(x_train, y2_train)))\n",
    "print('Bagging for Kernel SVM on test set: {:.2f}\\n'\n",
    "     .format(bag_knn.score(x_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN, Linear SVM and OLS have similar train and test scores. Looking across all methods, KNN seems to be consistent and hence we can use that to predict y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "691/691 [==============================] - 3s 4ms/step - loss: 696.5157 - mean_squared_error: 696.5157\n",
      "Epoch 2/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 690.9538 - mean_squared_error: 690.9538\n",
      "Epoch 3/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 673.6300 - mean_squared_error: 673.6300\n",
      "Epoch 4/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 628.9118 - mean_squared_error: 628.9118\n",
      "Epoch 5/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 543.5383 - mean_squared_error: 543.5383\n",
      "Epoch 6/100\n",
      "691/691 [==============================] - 0s 113us/step - loss: 417.1099 - mean_squared_error: 417.1099\n",
      "Epoch 7/100\n",
      "691/691 [==============================] - 0s 489us/step - loss: 271.9488 - mean_squared_error: 271.9488\n",
      "Epoch 8/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 152.9155 - mean_squared_error: 152.9155\n",
      "Epoch 9/100\n",
      "691/691 [==============================] - 0s 116us/step - loss: 90.3834 - mean_squared_error: 90.3834\n",
      "Epoch 10/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 68.8687 - mean_squared_error: 68.8687\n",
      "Epoch 11/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 60.7607 - mean_squared_error: 60.7607\n",
      "Epoch 12/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 54.4806 - mean_squared_error: 54.4806\n",
      "Epoch 13/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 48.9377 - mean_squared_error: 48.9377\n",
      "Epoch 14/100\n",
      "691/691 [==============================] - 0s 148us/step - loss: 44.0551 - mean_squared_error: 44.0551\n",
      "Epoch 15/100\n",
      "691/691 [==============================] - 0s 152us/step - loss: 39.6791 - mean_squared_error: 39.6791\n",
      "Epoch 16/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 35.7159 - mean_squared_error: 35.7159\n",
      "Epoch 17/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 32.2248 - mean_squared_error: 32.2248\n",
      "Epoch 18/100\n",
      "691/691 [==============================] - 0s 382us/step - loss: 29.1635 - mean_squared_error: 29.1635\n",
      "Epoch 19/100\n",
      "691/691 [==============================] - 0s 197us/step - loss: 26.5306 - mean_squared_error: 26.5306\n",
      "Epoch 20/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 24.2985 - mean_squared_error: 24.2985\n",
      "Epoch 21/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 22.4200 - mean_squared_error: 22.4200\n",
      "Epoch 22/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 20.8547 - mean_squared_error: 20.8547\n",
      "Epoch 23/100\n",
      "691/691 [==============================] - 0s 119us/step - loss: 19.5516 - mean_squared_error: 19.5516\n",
      "Epoch 24/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 18.4911 - mean_squared_error: 18.4911\n",
      "Epoch 25/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 17.6304 - mean_squared_error: 17.6304\n",
      "Epoch 26/100\n",
      "691/691 [==============================] - 0s 117us/step - loss: 16.9428 - mean_squared_error: 16.9428\n",
      "Epoch 27/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 16.4048 - mean_squared_error: 16.4048\n",
      "Epoch 28/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 15.9368 - mean_squared_error: 15.9368\n",
      "Epoch 29/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 15.5884 - mean_squared_error: 15.5884\n",
      "Epoch 30/100\n",
      "691/691 [==============================] - 0s 415us/step - loss: 15.2518 - mean_squared_error: 15.2518\n",
      "Epoch 31/100\n",
      "691/691 [==============================] - 0s 164us/step - loss: 15.0345 - mean_squared_error: 15.0345\n",
      "Epoch 32/100\n",
      "691/691 [==============================] - 0s 116us/step - loss: 14.7828 - mean_squared_error: 14.7828\n",
      "Epoch 33/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 14.5940 - mean_squared_error: 14.5940\n",
      "Epoch 34/100\n",
      "691/691 [==============================] - 0s 119us/step - loss: 14.3765 - mean_squared_error: 14.3765\n",
      "Epoch 35/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 14.2333 - mean_squared_error: 14.2333\n",
      "Epoch 36/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 14.1350 - mean_squared_error: 14.1350\n",
      "Epoch 37/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 13.9328 - mean_squared_error: 13.9328\n",
      "Epoch 38/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 13.7868 - mean_squared_error: 13.7868\n",
      "Epoch 39/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 13.6745 - mean_squared_error: 13.6745\n",
      "Epoch 40/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 13.5498 - mean_squared_error: 13.5498\n",
      "Epoch 41/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 13.4444 - mean_squared_error: 13.4444\n",
      "Epoch 42/100\n",
      "691/691 [==============================] - 0s 364us/step - loss: 13.4033 - mean_squared_error: 13.4033\n",
      "Epoch 43/100\n",
      "691/691 [==============================] - 0s 210us/step - loss: 13.2833 - mean_squared_error: 13.2833\n",
      "Epoch 44/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 13.1711 - mean_squared_error: 13.1711\n",
      "Epoch 45/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 13.0833 - mean_squared_error: 13.0833\n",
      "Epoch 46/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 13.0046 - mean_squared_error: 13.0046\n",
      "Epoch 47/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 12.9275 - mean_squared_error: 12.9275\n",
      "Epoch 48/100\n",
      "691/691 [==============================] - 0s 119us/step - loss: 12.8918 - mean_squared_error: 12.8918\n",
      "Epoch 49/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 12.8139 - mean_squared_error: 12.8139\n",
      "Epoch 50/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 12.7716 - mean_squared_error: 12.7716\n",
      "Epoch 51/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 12.7017 - mean_squared_error: 12.7017\n",
      "Epoch 52/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 12.7581 - mean_squared_error: 12.7581\n",
      "Epoch 53/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 12.5880 - mean_squared_error: 12.5880\n",
      "Epoch 54/100\n",
      "691/691 [==============================] - 0s 168us/step - loss: 12.5553 - mean_squared_error: 12.5553\n",
      "Epoch 55/100\n",
      "691/691 [==============================] - 0s 376us/step - loss: 12.5067 - mean_squared_error: 12.5067\n",
      "Epoch 56/100\n",
      "691/691 [==============================] - 0s 167us/step - loss: 12.4988 - mean_squared_error: 12.4988\n",
      "Epoch 57/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 12.4328 - mean_squared_error: 12.4328\n",
      "Epoch 58/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 12.4087 - mean_squared_error: 12.4087\n",
      "Epoch 59/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 12.4193 - mean_squared_error: 12.4193\n",
      "Epoch 60/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 12.3315 - mean_squared_error: 12.3315\n",
      "Epoch 61/100\n",
      "691/691 [==============================] - 0s 134us/step - loss: 12.3328 - mean_squared_error: 12.3328\n",
      "Epoch 62/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 12.3067 - mean_squared_error: 12.3067\n",
      "Epoch 63/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 12.2803 - mean_squared_error: 12.2803\n",
      "Epoch 64/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 12.2508 - mean_squared_error: 12.2508\n",
      "Epoch 65/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 12.1990 - mean_squared_error: 12.1990\n",
      "Epoch 66/100\n",
      "691/691 [==============================] - 0s 227us/step - loss: 12.1656 - mean_squared_error: 12.1656\n",
      "Epoch 67/100\n",
      "691/691 [==============================] - 0s 351us/step - loss: 12.1796 - mean_squared_error: 12.1796\n",
      "Epoch 68/100\n",
      "691/691 [==============================] - 0s 148us/step - loss: 12.1652 - mean_squared_error: 12.1652\n",
      "Epoch 69/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 12.1088 - mean_squared_error: 12.1088\n",
      "Epoch 70/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 12.0995 - mean_squared_error: 12.0995\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 138us/step - loss: 12.0692 - mean_squared_error: 12.0692\n",
      "Epoch 72/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 12.0361 - mean_squared_error: 12.0361\n",
      "Epoch 73/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 12.0300 - mean_squared_error: 12.0300\n",
      "Epoch 74/100\n",
      "691/691 [==============================] - 0s 113us/step - loss: 12.0224 - mean_squared_error: 12.0224\n",
      "Epoch 75/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 12.0016 - mean_squared_error: 12.0016\n",
      "Epoch 76/100\n",
      "691/691 [==============================] - 0s 113us/step - loss: 11.9737 - mean_squared_error: 11.9737\n",
      "Epoch 77/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 11.9275 - mean_squared_error: 11.9275\n",
      "Epoch 78/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 12.0240 - mean_squared_error: 12.0240\n",
      "Epoch 79/100\n",
      "691/691 [==============================] - 0s 482us/step - loss: 11.9758 - mean_squared_error: 11.9758\n",
      "Epoch 80/100\n",
      "691/691 [==============================] - 0s 100us/step - loss: 11.9277 - mean_squared_error: 11.9277\n",
      "Epoch 81/100\n",
      "691/691 [==============================] - 0s 107us/step - loss: 11.8942 - mean_squared_error: 11.8942\n",
      "Epoch 82/100\n",
      "691/691 [==============================] - 0s 167us/step - loss: 11.8641 - mean_squared_error: 11.8641\n",
      "Epoch 83/100\n",
      "691/691 [==============================] - 0s 157us/step - loss: 11.8300 - mean_squared_error: 11.8300\n",
      "Epoch 84/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 11.8791 - mean_squared_error: 11.8791\n",
      "Epoch 85/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 11.7994 - mean_squared_error: 11.7994\n",
      "Epoch 86/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 11.7836 - mean_squared_error: 11.7836\n",
      "Epoch 87/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 11.7788 - mean_squared_error: 11.7788\n",
      "Epoch 88/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 11.8426 - mean_squared_error: 11.8426\n",
      "Epoch 89/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 11.8374 - mean_squared_error: 11.8374\n",
      "Epoch 90/100\n",
      "691/691 [==============================] - 0s 263us/step - loss: 11.7536 - mean_squared_error: 11.7536\n",
      "Epoch 91/100\n",
      "691/691 [==============================] - 0s 328us/step - loss: 11.7203 - mean_squared_error: 11.7203\n",
      "Epoch 92/100\n",
      "691/691 [==============================] - 0s 119us/step - loss: 11.7568 - mean_squared_error: 11.7568\n",
      "Epoch 93/100\n",
      "691/691 [==============================] - 0s 116us/step - loss: 11.6857 - mean_squared_error: 11.6857\n",
      "Epoch 94/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 11.7265 - mean_squared_error: 11.7265\n",
      "Epoch 95/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 11.6579 - mean_squared_error: 11.6579\n",
      "Epoch 96/100\n",
      "691/691 [==============================] - 0s 152us/step - loss: 11.6673 - mean_squared_error: 11.6673\n",
      "Epoch 97/100\n",
      "691/691 [==============================] - 0s 161us/step - loss: 11.6194 - mean_squared_error: 11.6194\n",
      "Epoch 98/100\n",
      "691/691 [==============================] - 0s 182us/step - loss: 11.6308 - mean_squared_error: 11.6308\n",
      "Epoch 99/100\n",
      "691/691 [==============================] - 0s 199us/step - loss: 11.6330 - mean_squared_error: 11.6330\n",
      "Epoch 100/100\n",
      "691/691 [==============================] - 0s 189us/step - loss: 11.5648 - mean_squared_error: 11.5648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e44bf3c780>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=8, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(6, kernel_initializer = 'normal', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='adam' , metrics = ['mse'])\n",
    "\n",
    "model.fit(x_train, y2_train, epochs = 100, batch_size = 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.87\n",
      "Test r2: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y2_train_predict = model.predict(x_train)\n",
    "y2_test_predict = model.predict(x_test)\n",
    "\n",
    "print('Train r2: {:.2f}'.format(r2_score(y2_train, y2_train_predict)))\n",
    "print('Test r2: {:.2f}'.format(r2_score(y2_test, y2_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since KNN gives a high test score with comparable training score, we can use KNN to predict y2 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the overall load into three classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding heating and cooling loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = data\n",
    "data1['Total load'] = data1['Y1'] + data1['Y2']\n",
    "data2 = data1.drop([\"Y1\",\"Y2\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Total load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764167</td>\n",
       "      <td>671.708333</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>176.604167</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.81250</td>\n",
       "      <td>46.894961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105777</td>\n",
       "      <td>88.086116</td>\n",
       "      <td>43.626481</td>\n",
       "      <td>45.165950</td>\n",
       "      <td>1.75114</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.133221</td>\n",
       "      <td>1.55096</td>\n",
       "      <td>19.484947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>16.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.682500</td>\n",
       "      <td>606.375000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>140.875000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>28.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>673.750000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>183.750000</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>40.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>741.125000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>64.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>89.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4         X5          X6  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
       "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
       "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
       "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
       "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
       "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
       "\n",
       "               X7         X8  Total load  \n",
       "count  768.000000  768.00000  768.000000  \n",
       "mean     0.234375    2.81250   46.894961  \n",
       "std      0.133221    1.55096   19.484947  \n",
       "min      0.000000    0.00000   16.950000  \n",
       "25%      0.100000    1.75000   28.750000  \n",
       "50%      0.250000    3.00000   40.970000  \n",
       "75%      0.400000    4.00000   64.335000  \n",
       "max      0.400000    5.00000   89.950000  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First initialize the categorical binary matrix as 0s\n",
    "data2['Class1']=0\n",
    "data2['Class2']=0\n",
    "data2['Class3']=0\n",
    "\n",
    "#Then, activate the classification based on Y3 values\n",
    "data2.loc[(data2['Total load'] < 28.75),'Class3'] = 1 \n",
    "data2.loc[((data2['Total load'] >= 28.75) & (data2['Total load'] <= 64.335)),'Class2'] = 1 \n",
    "data2.loc[(data2['Total load'] > 64.335),'Class1'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Total load</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Class3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764167</td>\n",
       "      <td>671.708333</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>176.604167</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.81250</td>\n",
       "      <td>46.894961</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105777</td>\n",
       "      <td>88.086116</td>\n",
       "      <td>43.626481</td>\n",
       "      <td>45.165950</td>\n",
       "      <td>1.75114</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.133221</td>\n",
       "      <td>1.55096</td>\n",
       "      <td>19.484947</td>\n",
       "      <td>0.433295</td>\n",
       "      <td>0.500326</td>\n",
       "      <td>0.433295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>16.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.682500</td>\n",
       "      <td>606.375000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>140.875000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>673.750000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>183.750000</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>40.970000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>741.125000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>64.335000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>89.950000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4         X5          X6  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
       "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
       "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
       "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
       "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
       "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
       "\n",
       "               X7         X8  Total load      Class1      Class2      Class3  \n",
       "count  768.000000  768.00000  768.000000  768.000000  768.000000  768.000000  \n",
       "mean     0.234375    2.81250   46.894961    0.250000    0.500000    0.250000  \n",
       "std      0.133221    1.55096   19.484947    0.433295    0.500326    0.433295  \n",
       "min      0.000000    0.00000   16.950000    0.000000    0.000000    0.000000  \n",
       "25%      0.100000    1.75000   28.750000    0.000000    0.000000    0.000000  \n",
       "50%      0.250000    3.00000   40.970000    0.000000    0.500000    0.000000  \n",
       "75%      0.400000    4.00000   64.335000    0.250000    1.000000    0.250000  \n",
       "max      0.400000    5.00000   89.950000    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Classes using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data2[['X1','X2','X3','X4','X5','X6','X7','X8']]\n",
    "y = data2[['Class1','Class2','Class3']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.1,random_state=10)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "691/691 [==============================] - 3s 5ms/step - loss: 0.9669 - acc: 0.7115\n",
      "Epoch 2/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.5813 - acc: 0.7501\n",
      "Epoch 3/100\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.3845 - acc: 0.8061\n",
      "Epoch 4/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 0.3050 - acc: 0.8370\n",
      "Epoch 5/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.2748 - acc: 0.8548\n",
      "Epoch 6/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.2633 - acc: 0.8712\n",
      "Epoch 7/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 0.2562 - acc: 0.8722\n",
      "Epoch 8/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.2525 - acc: 0.8881\n",
      "Epoch 9/100\n",
      "691/691 [==============================] - 0s 329us/step - loss: 0.2490 - acc: 0.9026\n",
      "Epoch 10/100\n",
      "691/691 [==============================] - 0s 313us/step - loss: 0.2462 - acc: 0.9045\n",
      "Epoch 11/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.2428 - acc: 0.9117\n",
      "Epoch 12/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 0.2400 - acc: 0.9165\n",
      "Epoch 13/100\n",
      "691/691 [==============================] - 0s 154us/step - loss: 0.2384 - acc: 0.9170\n",
      "Epoch 14/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.2363 - acc: 0.9165\n",
      "Epoch 15/100\n",
      "691/691 [==============================] - 0s 154us/step - loss: 0.2351 - acc: 0.9137\n",
      "Epoch 16/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.2336 - acc: 0.9117\n",
      "Epoch 17/100\n",
      "691/691 [==============================] - 0s 155us/step - loss: 0.2320 - acc: 0.9137\n",
      "Epoch 18/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 0.2301 - acc: 0.9098\n",
      "Epoch 19/100\n",
      "691/691 [==============================] - 0s 157us/step - loss: 0.2287 - acc: 0.9122\n",
      "Epoch 20/100\n",
      "691/691 [==============================] - 0s 384us/step - loss: 0.2272 - acc: 0.9108\n",
      "Epoch 21/100\n",
      "691/691 [==============================] - 0s 203us/step - loss: 0.2265 - acc: 0.9156\n",
      "Epoch 22/100\n",
      "691/691 [==============================] - 0s 158us/step - loss: 0.2245 - acc: 0.9083\n",
      "Epoch 23/100\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.2233 - acc: 0.9122\n",
      "Epoch 24/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.2217 - acc: 0.9132\n",
      "Epoch 25/100\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.2140 - acc: 0.919 - 0s 145us/step - loss: 0.2208 - acc: 0.9137\n",
      "Epoch 26/100\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.2194 - acc: 0.9064\n",
      "Epoch 27/100\n",
      "691/691 [==============================] - 0s 164us/step - loss: 0.2180 - acc: 0.9112\n",
      "Epoch 28/100\n",
      "691/691 [==============================] - 0s 157us/step - loss: 0.2188 - acc: 0.9122\n",
      "Epoch 29/100\n",
      "691/691 [==============================] - 0s 155us/step - loss: 0.2163 - acc: 0.9108\n",
      "Epoch 30/100\n",
      "691/691 [==============================] - 0s 337us/step - loss: 0.2161 - acc: 0.9098\n",
      "Epoch 31/100\n",
      "691/691 [==============================] - 0s 331us/step - loss: 0.2146 - acc: 0.9079\n",
      "Epoch 32/100\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.2148 - acc: 0.9112\n",
      "Epoch 33/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 0.2118 - acc: 0.9103\n",
      "Epoch 34/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 0.2109 - acc: 0.9117\n",
      "Epoch 35/100\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.2098 - acc: 0.9141\n",
      "Epoch 36/100\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.2088 - acc: 0.9108\n",
      "Epoch 37/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.2076 - acc: 0.9151\n",
      "Epoch 38/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.2074 - acc: 0.9141\n",
      "Epoch 39/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.2058 - acc: 0.9132\n",
      "Epoch 40/100\n",
      "691/691 [==============================] - 0s 265us/step - loss: 0.2053 - acc: 0.9156\n",
      "Epoch 41/100\n",
      "691/691 [==============================] - 0s 338us/step - loss: 0.2047 - acc: 0.9132\n",
      "Epoch 42/100\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.2033 - acc: 0.9146\n",
      "Epoch 43/100\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.2017 - acc: 0.920 - 0s 141us/step - loss: 0.2032 - acc: 0.9151\n",
      "Epoch 44/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.2017 - acc: 0.9165\n",
      "Epoch 45/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.2013 - acc: 0.9112\n",
      "Epoch 46/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.2006 - acc: 0.9180\n",
      "Epoch 47/100\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.2006 - acc: 0.9079\n",
      "Epoch 48/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.1994 - acc: 0.9180\n",
      "Epoch 49/100\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.2023 - acc: 0.913 - 0s 148us/step - loss: 0.1985 - acc: 0.9141\n",
      "Epoch 50/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.1972 - acc: 0.9170\n",
      "Epoch 51/100\n",
      "691/691 [==============================] - 0s 329us/step - loss: 0.1966 - acc: 0.9146\n",
      "Epoch 52/100\n",
      "691/691 [==============================] - 0s 302us/step - loss: 0.1962 - acc: 0.9175\n",
      "Epoch 53/100\n",
      "691/691 [==============================] - 0s 152us/step - loss: 0.1957 - acc: 0.9194\n",
      "Epoch 54/100\n",
      "691/691 [==============================] - 0s 170us/step - loss: 0.1950 - acc: 0.9137\n",
      "Epoch 55/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.1947 - acc: 0.9194\n",
      "Epoch 56/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.1935 - acc: 0.9151\n",
      "Epoch 57/100\n",
      "691/691 [==============================] - 0s 152us/step - loss: 0.1929 - acc: 0.9175\n",
      "Epoch 58/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 0.1928 - acc: 0.9156\n",
      "Epoch 59/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.1914 - acc: 0.9194\n",
      "Epoch 60/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.1923 - acc: 0.9204\n",
      "Epoch 61/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.1918 - acc: 0.9156\n",
      "Epoch 62/100\n",
      "691/691 [==============================] - 0s 454us/step - loss: 0.1901 - acc: 0.9165\n",
      "Epoch 63/100\n",
      "691/691 [==============================] - 0s 151us/step - loss: 0.1892 - acc: 0.9190\n",
      "Epoch 64/100\n",
      "691/691 [==============================] - 0s 164us/step - loss: 0.1898 - acc: 0.9204\n",
      "Epoch 65/100\n",
      "691/691 [==============================] - 0s 167us/step - loss: 0.1891 - acc: 0.9175\n",
      "Epoch 66/100\n",
      "691/691 [==============================] - 0s 164us/step - loss: 0.1883 - acc: 0.9199\n",
      "Epoch 67/100\n",
      "691/691 [==============================] - 0s 190us/step - loss: 0.1882 - acc: 0.9112\n",
      "Epoch 68/100\n",
      "691/691 [==============================] - 0s 188us/step - loss: 0.1873 - acc: 0.9228\n",
      "Epoch 69/100\n",
      "691/691 [==============================] - 0s 239us/step - loss: 0.1883 - acc: 0.9108\n",
      "Epoch 70/100\n",
      "691/691 [==============================] - 0s 248us/step - loss: 0.1871 - acc: 0.9238\n",
      "Epoch 71/100\n",
      "691/691 [==============================] - 0s 221us/step - loss: 0.1863 - acc: 0.9180\n",
      "Epoch 72/100\n",
      "691/691 [==============================] - 0s 222us/step - loss: 0.1873 - acc: 0.9238\n",
      "Epoch 73/100\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.1847 - acc: 0.9209\n",
      "Epoch 74/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 0.1843 - acc: 0.9156\n",
      "Epoch 75/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.1845 - acc: 0.9204\n",
      "Epoch 76/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.1837 - acc: 0.9233\n",
      "Epoch 77/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.1838 - acc: 0.9151\n",
      "Epoch 78/100\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.1837 - acc: 0.9219\n",
      "Epoch 79/100\n",
      "691/691 [==============================] - 0s 165us/step - loss: 0.1830 - acc: 0.9156\n",
      "Epoch 80/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.1821 - acc: 0.9219\n",
      "Epoch 81/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.1814 - acc: 0.9194\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 280us/step - loss: 0.1812 - acc: 0.9141\n",
      "Epoch 83/100\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.1910 - acc: 0.918 - 0s 295us/step - loss: 0.1804 - acc: 0.9204\n",
      "Epoch 84/100\n",
      "691/691 [==============================] - 0s 197us/step - loss: 0.1800 - acc: 0.9165\n",
      "Epoch 85/100\n",
      "691/691 [==============================] - 0s 148us/step - loss: 0.1793 - acc: 0.9175\n",
      "Epoch 86/100\n",
      "691/691 [==============================] - 0s 126us/step - loss: 0.1796 - acc: 0.9156\n",
      "Epoch 87/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 0.1790 - acc: 0.9233\n",
      "Epoch 88/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.1783 - acc: 0.9209\n",
      "Epoch 89/100\n",
      "691/691 [==============================] - 0s 178us/step - loss: 0.1788 - acc: 0.9141\n",
      "Epoch 90/100\n",
      "691/691 [==============================] - 0s 170us/step - loss: 0.1773 - acc: 0.9165\n",
      "Epoch 91/100\n",
      "691/691 [==============================] - 0s 167us/step - loss: 0.1777 - acc: 0.9199\n",
      "Epoch 92/100\n",
      "691/691 [==============================] - 0s 209us/step - loss: 0.1774 - acc: 0.9209\n",
      "Epoch 93/100\n",
      "691/691 [==============================] - 0s 312us/step - loss: 0.1765 - acc: 0.9190\n",
      "Epoch 94/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.1771 - acc: 0.9161\n",
      "Epoch 95/100\n",
      "691/691 [==============================] - 0s 175us/step - loss: 0.1762 - acc: 0.9209\n",
      "Epoch 96/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 0.1763 - acc: 0.9141\n",
      "Epoch 97/100\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.1771 - acc: 0.9223\n",
      "Epoch 98/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.1756 - acc: 0.9151\n",
      "Epoch 99/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.1755 - acc: 0.9214\n",
      "Epoch 100/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.1748 - acc: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e44750d080>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 130us/step\n",
      "\n",
      "acc: 90.48%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 59us/step\n",
      "\n",
      "acc: 91.80%\n"
     ]
    }
   ],
   "source": [
    "scores1 = model.evaluate(x_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model gives a training accuracy of 91.8% and a test accuracy of 90.48% which indicates that the model will do a good job at predicting the classes for new values of heating and cooling load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "#### Heating Load prediction - Base ML methods - OLS and Linear SVR gave very good results and turned out to be good models. Neural networks gave a very similar score and hence we can choose any one amongst them. Different values of batch size and epochs may give us a better result.\n",
    "#### Heating Load prediction - Base ML methods - KNN gave very good results compared to other models. Neural networks gave a slightly lower training and test score. Different values of batch size and epochs may give us a better result.\n",
    "#### Combined Load Multi-Class prediction: Almost equal training and test scores. Scores can be increased by tweaking values of neuron layers and other parameters as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
